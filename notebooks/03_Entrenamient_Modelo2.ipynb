{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"Explorando el corazón: Utilizando datos clínicos y de estilo de vida para predecir enfermedades cardíacas\"\n",
    "\n",
    "#### ENTRENAMIENTO DE LOS MODELOS\n",
    "\n",
    "A lo largo de este notebook, abordaremos el problema de clasificación y nos enfocaremos en entrenar varios modelos con el objetivo de obtener las métricas más óptimas para nuestro problema. Nuestro objetivo final es encontrar el modelo que brinde el mejor rendimiento en la clasificación de los datos. A través de este proceso de entrenamiento y evaluación, exploraremos diferentes algoritmos y técnicas para optimizar nuestros modelos y lograr resultados confiables y eficientes. Estaremos atentos a métricas como la precisión, la exactitud, el puntaje F1 y otras medidas relevantes para evaluar el desempeño de cada modelo. Al final de este notebook, esperamos haber identificado el modelo más adecuado que se ajuste a nuestras necesidades y nos proporcione las mejores métricas en nuestra tarea de clasificación.\n",
    "\n",
    "1. Logistic Regresion\n",
    "2. Naive Bayes\n",
    "3. Decision Tree Classifier\n",
    "4. Random Forest Classifier\n",
    "5. XGB Classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las distintas librerias necesarias para el análisis\n",
    "\n",
    "# Tratamiento de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,log_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Para guardar los modelos\n",
    "import pickle\n",
    "\n",
    "# Configuración warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory_encoded</th>\n",
       "      <th>Diabetic_encoded</th>\n",
       "      <th>BMI_Category_Ordinal</th>\n",
       "      <th>GrupoSalud_Ordinal</th>\n",
       "      <th>GrupoSalud_Mental_Ordinal</th>\n",
       "      <th>SleepGroup_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease  Smoking  Stroke  DiffWalking  Sex  AgeCategory_encoded   \n",
       "0             0        1       0            0    0                    7  \\\n",
       "1             0        0       1            0    0                   12   \n",
       "2             0        1       0            0    1                    9   \n",
       "3             0        0       0            0    0                   11   \n",
       "4             0        0       0            1    0                    4   \n",
       "\n",
       "   Diabetic_encoded  BMI_Category_Ordinal  GrupoSalud_Ordinal   \n",
       "0                 2                     1                   1  \\\n",
       "1                 0                     2                   1   \n",
       "2                 2                     3                   3   \n",
       "3                 0                     2                   1   \n",
       "4                 0                     2                   3   \n",
       "\n",
       "   GrupoSalud_Mental_Ordinal  SleepGroup_Ordinal  \n",
       "0                          3                   1  \n",
       "1                          1                   3  \n",
       "2                          3                   3  \n",
       "3                          1                   2  \n",
       "4                          1                   3  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el archivo de data/preprocessed_heart\n",
    "df= pd.read_csv(\"../data/processed/processed_heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease                 100.000000\n",
       "AgeCategory_encoded           23.343224\n",
       "DiffWalking                   20.125805\n",
       "Stroke                        19.683530\n",
       "Diabetic_encoded              16.855285\n",
       "GrupoSalud_Ordinal            16.731876\n",
       "Smoking                       10.776416\n",
       "Sex                            7.004048\n",
       "BMI_Category_Ordinal           5.342468\n",
       "GrupoSalud_Mental_Ordinal      2.130066\n",
       "SleepGroup_Ordinal            -3.456049\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para observar mejor la correlación que se produce con respecto a la varible target de \"HeartDisease\"\n",
    "# Se muestra en porcentage % y por orden ascendente\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix['HeartDisease'].sort_values(ascending=False) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease                 2.962525\n",
       "Smoking                      0.355585\n",
       "Stroke                       4.851460\n",
       "DiffWalking                  2.088606\n",
       "Sex                          0.099029\n",
       "AgeCategory_encoded         -0.263611\n",
       "Diabetic_encoded             2.088800\n",
       "BMI_Category_Ordinal         0.659454\n",
       "GrupoSalud_Ordinal           2.199359\n",
       "GrupoSalud_Mental_Ordinal    1.808620\n",
       "SleepGroup_Ordinal          -1.391516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos skew para calcular la asimetría de nuestro DataFrame\n",
    "    # Un valor positivo indica una cola más larga hacia la derecha, lo que significa que hay valores extremos más altos. \n",
    "    # Un valor negativo indica una cola más larga hacia la izquierda, lo que significa que hay valores extremos más bajos.\n",
    "df.skew(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease\n",
      "0    292422\n",
      "1     27373\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Smoking\n",
      "0    187887\n",
      "1    131908\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Stroke\n",
      "0    307726\n",
      "1     12069\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DiffWalking\n",
      "0    275385\n",
      "1     44410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sex\n",
      "0    167805\n",
      "1    151990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AgeCategory_encoded\n",
      "9     34151\n",
      "8     33686\n",
      "10    31065\n",
      "7     29757\n",
      "6     25382\n",
      "12    24153\n",
      "5     21791\n",
      "11    21482\n",
      "0     21064\n",
      "4     21006\n",
      "3     20550\n",
      "2     18753\n",
      "1     16955\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Diabetic_encoded\n",
      "0    269653\n",
      "2     40802\n",
      "1      6781\n",
      "3      2559\n",
      "Name: count, dtype: int64\n",
      "\n",
      "BMI_Category_Ordinal\n",
      "3    113640\n",
      "2     88661\n",
      "4     67157\n",
      "5     27379\n",
      "6     17844\n",
      "1      5114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GrupoSalud_Ordinal\n",
      "1    265043\n",
      "2     28748\n",
      "3     26004\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GrupoSalud_Mental_Ordinal\n",
      "1    247032\n",
      "2     45891\n",
      "3     26872\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SleepGroup_Ordinal\n",
      "3    222809\n",
      "2     66721\n",
      "1     30265\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hacemos un bucle para comprobar todas las columnas cuales están desbalanceadas y cuales no\n",
    "for column in df.columns:\n",
    "    print(df[column].value_counts(sort=True))  # Contamos los valores de la columna y mostramos los resultados ordenados\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas que no necesitaremos más\n",
    "df = df.drop(columns=([\"SleepGroup_Ordinal\", \"GrupoSalud_Mental_Ordinal\",\"BMI_Category_Ordinal\", ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos nuestras etiquetas y features\n",
    "y = df['HeartDisease']\n",
    "X = df.drop('HeartDisease', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Smoking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory_encoded',\n",
       "       'Diabetic_encoded', 'GrupoSalud_Ordinal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (255836, 7)\n",
      "X_test (63959, 7)\n",
      "y_train (255836,)\n",
      "y_test (63959,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que dividir train y test y guardarlo en sus resptivas carpetas dentro de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probamos el modelo sin aplicar el desbalanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar las características \n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42, solver='newton-cg')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logre = LogisticRegression(C=1.0,penalty='l2',random_state=42,solver=\"newton-cg\")\n",
    "logre.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Sin_Balanceo = logre.predict(X_test_scaled) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.9125689895089042\n",
      "Precisión por clase: 0.5\n",
      "Recall por clase: 0.08369098712446352\n",
      "Puntuación F1 por clase: 0.14338235294117646\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Suponiendo que ya has realizado todas las operaciones previas y tienes las variables adecuadas\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Precisión del modelo:\", accuracy)\n",
    "\n",
    "# Calcular la precisión por clase\n",
    "precision = precision_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Precisión por clase:\", precision)\n",
    "\n",
    "# Calcular el recall (sensibilidad) por clase\n",
    "recall = recall_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Recall por clase:\", recall)\n",
    "\n",
    "# Calcular la puntuación F1 por clase\n",
    "f1 = f1_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Puntuación F1 por clase:\", f1)\n",
    "\n",
    "# Calcular el classification report\n",
    "report = classification_report(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "Precisión del modelo: 0.9137785467849363. Esta métrica representa la proporción de predicciones correctas sobre el total de predicciones realizadas. En este caso, el modelo tiene una precisión del 91.38%, lo que significa que aproximadamente el 91.38% de las predicciones son correctas.\n",
    "\n",
    "Precisión por clase: 0.5062068965517241. Esta métrica representa la proporción de predicciones positivas que son correctas para la clase \"1\" (enfermedad cardíaca) en relación con todas las predicciones positivas realizadas para esa clase. Aquí, la precisión para la clase \"1\" es del 50.62%, lo que indica que alrededor del 50.62% de las predicciones positivas para la enfermedad cardíaca son correctas.\n",
    "\n",
    "Recall por clase: 0.08854041013268998. Esta métrica, también conocida como sensibilidad o tasa de verdaderos positivos, representa la proporción de casos positivos (enfermedad cardíaca) que son correctamente identificados por el modelo. En este caso, el recall para la clase \"1\" es del 8.85%, lo que significa que el modelo identifica correctamente aproximadamente el 8.85% de los casos reales de enfermedad cardíaca.\n",
    "\n",
    "Puntuación F1 por clase: 0.15071868583162218. La puntuación F1 es una medida que combina la precisión y el recall en una única métrica. Es útil cuando hay un desequilibrio entre las clases, como en este caso. La puntuación F1 para la clase \"1\" es del 15.07%, lo que indica un equilibrio entre la precisión y el recall para esa clase.\n",
    "\n",
    "En cuanto al classification report, proporciona un resumen detallado de las métricas por clase y para el conjunto de datos completo. Observamos que la clase \"0\" (no enfermedad cardíaca) tiene una precisión alta, recall alto y puntuación F1 alta, mientras que la clase \"1\" (enfermedad cardíaca) muestra valores más bajos en todas las métricas. Esto puede indicar un desequilibrio en los datos, donde la clase \"1\" tiene menos representación en comparación con la clase \"0\". El conjunto de datos está sesgado hacia la clase \"0\" y eso puede afectar el rendimiento del modelo en la detección de la clase minoritaria (enfermedad cardíaca).\n",
    "\n",
    "En resumen, las métricas indican que el modelo tiene una precisión general alta, pero tiene dificultades para identificar correctamente los casos de enfermedad cardíaca. Esto puede deberse al desequilibrio en los datos y es importante tenerlo en cuenta al interpretar los resultados y tomar decisiones basadas en ellos.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[57899   468]\n",
      " [ 5124   468]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión \n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred_Sin_Balanceo) \n",
    "print(\"Matriz de confusión:\") \n",
    "print(confusion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report = classification_report(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_SinBalanceo = y_pred_Sin_Balanceo\n",
    "with open('../models/trained_model_Sin_Balanceo_Reg_Log.pkl', 'wb') as file:\n",
    "    pickle.dump(modelo_SinBalanceo, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APLICACIÓN DE LAS DISTINTAS TÉCNICAS PARA DESBALANCEAR EL PROBLEMA\n",
    "En esta sección, aplicaremos diferentes técnicas de desbalanceo de datos utilizando el mismo modelo con el objetivo de determinar la estrategia más efectiva. Evaluaremos las siguientes estrategias:\n",
    "\n",
    "<details>\n",
    "<summary>Técnicas utilizadas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "**Penalización para compensar**: Esta estrategia implica la aplicación de penalizaciones en el algoritmo de aprendizaje automático para compensar el desequilibrio en los datos. Se le asigna un peso mayor a las instancias de la clase minoritaria durante el entrenamiento para asegurar que se les dé más importancia en la predicción final.\n",
    "\n",
    "**Oversampling de la clase minoritaria**: En esta estrategia, generamos nuevas muestras sintéticas de la clase minoritaria para igualar la cantidad de muestras con la clase mayoritaria. Esto se logra replicando o generando nuevas instancias de la clase minoritaria en el conjunto de datos.\n",
    "\n",
    "**Combinación de resampling con Smote-Tomek:** Esta estrategia combina el undersampling y el oversampling para abordar el desbalanceo. El undersampling se aplica para reducir la cantidad de muestras de la clase mayoritaria, mientras que el oversampling se utiliza para generar nuevas muestras sintéticas de la clase minoritaria. Smote-Tomek es una técnica específica que combina el algoritmo SMOTE (Synthetic Minority Over-sampling Technique) con el método Tomek, que elimina instancias cercanas entre las dos clases.\n",
    "\n",
    "**Ensamble de Modelos con Balanceo**: En esta estrategia, utilizamos un ensamble de modelos para abordar el desbalanceo. Se entrenan varios modelos de aprendizaje automático utilizando diferentes técnicas de balanceo de datos. Luego, se combinan las predicciones de estos modelos para obtener una predicción final más robusta y equilibrada.\n",
    "\n",
    "Al evaluar estas estrategias y comparar su desempeño, podremos determinar cuál es la más adecuada para abordar el desbalanceo en nuestros datos y mejorar la calidad de nuestro modelo de aprendizaje automático.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "from pylab import rcParams\n",
    " \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    " \n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Penalización para compensar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos un parámetro adicional en el modelo de Regresión logística en donde indicamos weight = “balanced” y con esto el algoritmo se encargará de equilibrar a la clase minoritaria durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# EscalaMOS las características \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos \n",
    "\n",
    "# Modelo de Logistic Regression\n",
    "logre2 = LogisticRegression(C=0.1,penalty='l1',random_state=42,solver=\"liblinear\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "logre2.fit(X_train_scaled, y_train)\n",
    "y_pred2 = logre2.predict(X_test_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "En comparación con el modelo anterior, el nuevo modelo de Logistic Regression con los datos de prueba actualizados muestra una mejora en ciertas métricas, mientras que otras disminuyen. Aquí hay un resumen de las diferencias clave:\n",
    "\n",
    "Precisión general: El nuevo modelo tiene una precisión general de 0.74, lo que indica que aproximadamente el 74% de las predicciones son correctas. Esta precisión es menor que la del modelo anterior, que era de 0.91.\n",
    "\n",
    "Precisión por clase: Para la clase \"0\" (no enfermedad cardíaca), la precisión es alta con un valor de 0.97, lo que indica que la mayoría de las predicciones negativas son correctas. Sin embargo, para la clase \"1\" (enfermedad cardíaca), la precisión es baja con un valor de 0.22, lo que indica que la mayoría de las predicciones positivas son incorrectas.\n",
    "\n",
    "Recall por clase: El nuevo modelo muestra un recall alto para la clase \"1\" (enfermedad cardíaca) con un valor de 0.77, lo que indica que la mayoría de los casos reales de enfermedad cardíaca son correctamente identificados por el modelo. Sin embargo, el recall para la clase \"0\" (no enfermedad cardíaca) es de 0.74, lo que indica que hay algunos casos reales de no enfermedad cardíaca que no son identificados por el modelo.\n",
    "\n",
    "Puntuación F1 por clase: La puntuación F1 para la clase \"1\" (enfermedad cardíaca) es de 0.34, lo que indica un equilibrio entre la precisión y el recall para esa clase. Sin embargo, la puntuación F1 para la clase \"0\" (no enfermedad cardíaca) es de 0.84, lo que indica un mejor equilibrio entre precisión y recall para esa clase.\n",
    "\n",
    "En general, el nuevo modelo muestra una mejora en la identificación de casos de enfermedad cardíaca, como se refleja en un recall más alto para la clase \"1\". Sin embargo, la precisión para la clase \"1\" ha disminuido, lo que significa que hay un mayor número de falsos positivos en comparación con el modelo anterior. Además, la precisión general y la puntuación F1 han disminuido. Estos resultados sugieren que el modelo todavía puede beneficiarse de ajustes o considerar técnicas adicionales para abordar el desequilibrio en los datos y mejorar la detección de enfermedades cardíacas.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42480 15887]\n",
      " [ 1280  4312]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion2 = confusion_matrix(y_test, y_pred2)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Oversampling de la clase minoritaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, crearemos muestras nuevas “sintéticas” de la clase minoritaria. Usando RandomOverSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 117027})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar RandomOverSampler\n",
    "os = RandomOverSampler(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Imprimir distribución antes y después del resampling\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model = LogisticRegression(C=0.1,penalty='l2',random_state=42,solver=\"newton-cg\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "pred_y_random = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report4 = classification_report(y_test, pred_y_random)\n",
    "print(\"Classification Report:\")\n",
    "print(report4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42593 15774]\n",
      " [ 1302  4290]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion4 = confusion_matrix(y_test, pred_y_random)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Combinamos resampling con Smote-Tomek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probaremos una técnica muy usada que consiste en aplicar en simultáneo un algoritmo de subsampling y otro de oversampling a la vez al dataset. En este caso usaremos SMOTE para oversampling: busca puntos vecinos cercanos y agrega puntos “en linea recta” entre ellos. Y usaremos Tomek para undersampling que quita los de distinta clase que sean nearest neighbor y deja ver mejor el decisión boundary (la zona limítrofe de nuestras clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 233355})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from collections import Counter\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE()\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Aplicar Tomek Links para undersampling\n",
    "tomek = TomekLinks()\n",
    "X_train_res, y_train_res = tomek.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Entrenar un modelo (reemplaza esta parte con tu propio código de entrenamiento de modelo)\n",
    "model = LogisticRegression(C=1.0, penalty='l2', random_state=42, solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "pred_y = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.78      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.74      0.56     63959\n",
      "weighted avg       0.90      0.70      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report3 = classification_report(y_test, pred_y)\n",
    "print(\"Classification Report:\")\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[40667 17700]\n",
      " [ 1252  4340]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion3 = confusion_matrix(y_test, pred_y)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Ensamble de Modelos con Balanceo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estrategia usaremos un Clasificador de Ensamble que utiliza Bagging y el modelo será un DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=True,\n",
    "                                random_state=0,)\n",
    " \n",
    "#Train the classifier.\n",
    "bbc.fit(X_train, y_train)\n",
    "pred_yBagging = bbc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report4 = classification_report(y_test, pred_yBagging)\n",
    "print(\"Classification Report:\")\n",
    "print(report4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[40827 17540]\n",
      " [ 1118  4474]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion4 = confusion_matrix(y_test, pred_yBagging)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de las Estrategias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcula los pesos de clase\n",
    "total_samples = len(y)\n",
    "class_weights = total_samples / (len(np.unique(y)) * np.bincount(y))\n",
    "\n",
    "# Crea un diccionario de pesos de clase\n",
    "class_weight_dict = {class_index: weight for class_index, weight in enumerate(class_weights)}\n",
    "\n",
    "# Compila el modelo utilizando los pesos de clase\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['recall'], class_weight=class_weight_dict)\n",
    "\n",
    "# Entrena el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELOS A TRATAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de este apartado realizaremos la solución de problema a través de diferentes modelos supervisados entre los que nos encontramos con..... en busca de las mejores métricas posibles para nuestro problema......."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO SUPERVISADO\n",
    "<details>\n",
    "<summary>¿Qué es el aprendizaje supervisado?</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>¿Qué modelos vamos a tratar?</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline con escalamiento y modelo Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'logre2__C': [0.1, 1, 10], # regularización\n",
    "    'logre2__penalty': ['l1', 'l2'], # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # se prueba cada algoritmo\n",
    "    'logre2__class_weight': [None, 'balanced'],  # sin balanceo y con balanceo\n",
    "    'selectkbest__k': [5, 6, 7] # se seleccionan los mejores k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall') # He probado con varios cv y el cv=5 es el que mejor se adapta\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_logRe = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.69      0.81     58367\n",
      "           1       0.19      0.77      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.73      0.56     63959\n",
      "weighted avg       0.90      0.70      0.76     63959\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "{'logre2__C': 0.1, 'logre2__class_weight': 'balanced', 'logre2__penalty': 'l1', 'logre2__solver': 'liblinear', 'selectkbest__k': 5}\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el classification report\n",
    "report2 = classification_report(y_test, y_pred_logRe)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params_logRe = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params_logRe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo_logRe = y_pred_logRe\n",
    "with open('../models/trained_model_Log_Regression.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_logRe, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline con escalado, selección de características, SMOTE y modelo Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('smote', SMOTE()),  # Aplicar oversampling con SMOTE\n",
    "    ('naive_bayes', GaussianNB())\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'selectkbest__k': [7,8,9],\n",
    "    'smote__sampling_strategy': [0.5, 0.75, 1.0],  # Ajustar la proporción de oversampling\n",
    "    'naive_bayes__var_smoothing': [1e-9,1e-10,1e-11]  # Ajustar el suavizado de varianza \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 270 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=0.5)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=0.75)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=1.0)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\u001b[39;00m\n\u001b[0;32m      2\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, parameters, cv\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Obtenemos las predicciones del mejor modelo encontrado\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred_Naive \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 270 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=0.5)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=0.75)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 339, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alexm\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py\", line 230, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(sampling_strategy=1.0)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't\n"
     ]
    }
   ],
   "source": [
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=10, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_Naive = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87     58367\n",
      "           1       0.23      0.60      0.33      5592\n",
      "\n",
      "    accuracy                           0.79     63959\n",
      "   macro avg       0.59      0.70      0.60     63959\n",
      "weighted avg       0.89      0.79      0.82     63959\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "{'naive_bayes__var_smoothing': 1e-10, 'selectkbest__k': 7, 'smote__sampling_strategy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculmosr el classification report\n",
    "report3 = classification_report(y_test, y_pred_Naive)\n",
    "print(\"Classification Report:\")\n",
    "print(report3)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params_Naive = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params_Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo_Naive = y_pred_Naive\n",
    "with open('../models/trained_model_NaiveBayes.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_Naive, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISSION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\u001b[39;00m\n\u001b[0;32m     25\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, parameters, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Obtener las mejores configuraciones de hiperparámetros encontradas\u001b[39;00m\n\u001b[0;32m     29\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\ensemble\\_bagging.py:412\u001b[0m, in \u001b[0;36mBalancedBaggingClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m# overwrite the base class method by disallowing `sample_weight`\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 412\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:337\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[39m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    329\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m     multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    336\u001b[0m )\n\u001b[1;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\ensemble\\_bagging.py:427\u001b[0m, in \u001b[0;36mBalancedBaggingClassifier._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler_ \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler)\n\u001b[0;32m    425\u001b[0m \u001b[39m# RandomUnderSampler is not supporting sample_weight. We need to pass\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39m# None.\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples, sample_weight\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:472\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    469\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39mn_more_estimators)\n\u001b[0;32m    470\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seeds \u001b[39m=\u001b[39m seeds\n\u001b[1;32m--> 472\u001b[0m all_results \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    473\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parallel_args()\n\u001b[0;32m    474\u001b[0m )(\n\u001b[0;32m    475\u001b[0m     delayed(_parallel_build_estimators)(\n\u001b[0;32m    476\u001b[0m         n_estimators[i],\n\u001b[0;32m    477\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    478\u001b[0m         X,\n\u001b[0;32m    479\u001b[0m         y,\n\u001b[0;32m    480\u001b[0m         sample_weight,\n\u001b[0;32m    481\u001b[0m         seeds[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    482\u001b[0m         total_n_estimators,\n\u001b[0;32m    483\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    484\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    486\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_jobs)\n\u001b[0;32m    487\u001b[0m )\n\u001b[0;32m    489\u001b[0m \u001b[39m# Reduce\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    491\u001b[0m     itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(t[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m all_results)\n\u001b[0;32m    492\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:144\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     X_ \u001b[39m=\u001b[39m X[indices][:, features] \u001b[39mif\u001b[39;00m requires_feature_indexing \u001b[39melse\u001b[39;00m X[indices]\n\u001b[1;32m--> 144\u001b[0m     estimator_fit(X_, y[indices])\n\u001b[0;32m    146\u001b[0m estimators\u001b[39m.\u001b[39mappend(estimator)\n\u001b[0;32m    147\u001b[0m estimators_features\u001b[39m.\u001b[39mappend(features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\pipeline.py:293\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    292\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 293\u001b[0m Xt, yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    294\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\pipeline.py:250\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    240\u001b[0m     X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    241\u001b[0m         cloned_transformer,\n\u001b[0;32m    242\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    249\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(cloned_transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_resample\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 250\u001b[0m     X, y, fitted_transformer \u001b[39m=\u001b[39m fit_resample_one_cached(\n\u001b[0;32m    251\u001b[0m         cloned_transformer,\n\u001b[0;32m    252\u001b[0m         X,\n\u001b[0;32m    253\u001b[0m         y,\n\u001b[0;32m    254\u001b[0m         message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    255\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    256\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    258\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\pipeline.py:422\u001b[0m, in \u001b[0;36m_fit_resample_one\u001b[1;34m(sampler, X, y, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_resample_one\u001b[39m(sampler, X, y, message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m    421\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m--> 422\u001b[0m         X_res, y_res \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39mfit_resample(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m X_res, y_res, sampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\base.py:88\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     82\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m     86\u001b[0m )\n\u001b[1;32m---> 88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_resample(X, y)\n\u001b[0;32m     90\u001b[0m y_ \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     label_binarize(output[\u001b[39m1\u001b[39m], classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y)) \u001b[39mif\u001b[39;00m binarize_y \u001b[39melse\u001b[39;00m output[\u001b[39m1\u001b[39m]\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     94\u001b[0m X_, y_ \u001b[39m=\u001b[39m arrays_transformer\u001b[39m.\u001b[39mtransform(output[\u001b[39m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_random_under_sampler.py:118\u001b[0m, in \u001b[0;36mRandomUnderSampler._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m target_class \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    117\u001b[0m     n_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_[target_class]\n\u001b[1;32m--> 118\u001b[0m     index_target_class \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39;49mchoice(\n\u001b[0;32m    119\u001b[0m         \u001b[39mrange\u001b[39;49m(np\u001b[39m.\u001b[39;49mcount_nonzero(y \u001b[39m==\u001b[39;49m target_class)),\n\u001b[0;32m    120\u001b[0m         size\u001b[39m=\u001b[39;49mn_samples,\n\u001b[0;32m    121\u001b[0m         replace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplacement,\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     index_target_class \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# TARDA MUCHO NO EJECUTAR\n",
    "\n",
    "# Crear el pipeline con SMOTE, BalancedBaggingClassifier y DecisionTreeClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('bbc', BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                      random_state=0,\n",
    "                                      sampling_strategy='auto'))\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'bbc__n_estimators': [50, 80, 100],\n",
    "    'bbc__base_estimator__max_depth': [5, 8, 10],\n",
    "    'bbc__base_estimator__min_samples_split': [2, 5, 10],\n",
    "    'bbc__base_estimator__min_samples_leaf': [1, 2, 4],\n",
    "    'bbc__base_estimator__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros:\n",
      "{'bbc__base_estimator__max_depth': 10, 'bbc__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/trained_model.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo, file) # reemplaza mejor_modelo con el nombre de la variable que contiene el mejor modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Crear el pipeline con DecisionTreeClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('dt', DecisionTreeClassifier(random_state=42, class_weight='balanced')) # Para solucionar el problema de balanceo class_weight='balanced'\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'dt__max_depth': [5, 8, 10],\n",
    "    'dt__min_samples_split': [2, 5, 10],\n",
    "    'dt__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params = grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     58367\n",
      "           1       0.18      0.81      0.30      5592\n",
      "\n",
      "    accuracy                           0.66     63959\n",
      "   macro avg       0.58      0.73      0.54     63959\n",
      "weighted avg       0.90      0.66      0.74     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "pipeline.set_params(**best_params) # ** se utiliza para desempaquetar un diccionario y pasar sus elementos como argumentos de palabras clave a una función o método\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las predicciones\n",
    "y_pred_decisiontree = pipeline.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_decisiontree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo = y_pred_decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/trained_model_decisionTree.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo, file) # reemplaza mejor_modelo con el nombre de la variable que contiene el mejor modelo entrenado.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={&#x27;rfc__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                                        &#x27;rfc__max_depth&#x27;: [5, 8, 10],\n",
       "                                        &#x27;rfc__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [50, 80, 100]},\n",
       "                   random_state=0, scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={&#x27;rfc__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                                        &#x27;rfc__max_depth&#x27;: [5, 8, 10],\n",
       "                                        &#x27;rfc__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [50, 80, 100]},\n",
       "                   random_state=0, scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rfc&#x27;, RandomForestClassifier(random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('rfc',\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={'rfc__class_weight': ['balanced', None],\n",
       "                                        'rfc__max_depth': [5, 8, 10],\n",
       "                                        'rfc__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rfc__min_samples_split': [2, 5, 10],\n",
       "                                        'rfc__n_estimators': [50, 80, 100]},\n",
       "                   random_state=0, scoring='recall')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Crear el pipeline con Random Forest Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('rfc', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el RandomizedSearchCV\n",
    "parameters = {\n",
    "    'rfc__n_estimators': [50, 80, 100],\n",
    "    'rfc__max_depth': [5, 8, 10],\n",
    "    'rfc__min_samples_split': [2, 5, 10],\n",
    "    'rfc__min_samples_leaf': [1, 2, 4],\n",
    "    'rfc__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda aleatoria de hiperparámetros utilizando RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, parameters, cv=5, scoring='recall', random_state=0)\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__n_estimators': 100,\n",
       " 'rfc__min_samples_split': 10,\n",
       " 'rfc__min_samples_leaf': 2,\n",
       " 'rfc__max_depth': 8,\n",
       " 'rfc__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_RandomForest = random_search.best_params_\n",
    "best_params_RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_RandomForest = random_search.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_RandomForest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo_RandomForest = y_pred_decisiontree\n",
    "with open('../models/trained_model_RandomForest.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_RandomForest, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAUSSIANNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBCCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Crear el pipeline con escalado y XGBoost Classifier con scale_pos_weight\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=0, scale_pos_weight=3)) # scale_pos_weight=3 para el balanceo\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'xgb__max_depth': [2, 3, 4],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;xgb&#x27;,\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={&#x27;xgb__learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;xgb__max_depth&#x27;: [2, 3, 4]},\n",
       "             scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;xgb&#x27;,\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={&#x27;xgb__learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;xgb__max_depth&#x27;: [2, 3, 4]},\n",
       "             scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;xgb&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=0, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('xgb',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={'xgb__learning_rate': [0.1, 0.01, 0.001],\n",
       "                         'xgb__max_depth': [2, 3, 4]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search_XGB = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgb__learning_rate': 0.1, 'xgb__max_depth': 4}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_XGB = grid_search_XGB.best_params_\n",
    "best_params_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     58367\n",
      "           1       0.36      0.33      0.34      5592\n",
      "\n",
      "    accuracy                           0.89     63959\n",
      "   macro avg       0.65      0.64      0.64     63959\n",
      "weighted avg       0.89      0.89      0.89     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_XGB = grid_search_XGB.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo_XGB = y_pred_XGB\n",
    "with open('../models/trained_model_XGB_Classifiert.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_XGB, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO NO SUPERVISADO\n",
    "<details>\n",
    "<summary>¿Qué es el aprendizaje no supervisado?</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN CLASSIIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Crear el pipeline con escalamiento, PCA, selección de características y modelo Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'pca__n_components': [2, 3, 4],  # Número de componentes para PCA\n",
    "    'logre2__C': [0.1, 1, 10],  # Regularización\n",
    "    'logre2__penalty': ['l1', 'l2'],  # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # Algoritmos de optimización\n",
    "    'logre2__class_weight': [None, 'balanced'],  # Sin balanceo y con balanceo\n",
    "    'selectkbest__k': [3, 4, 5]  # Seleccionar los mejores k\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                                       (&#x27;logre2&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logre2__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logre2__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;logre2__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logre2__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;,\n",
       "                                            &#x27;sag&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;pca__n_components&#x27;: [2, 3, 4],\n",
       "                         &#x27;selectkbest__k&#x27;: [3, 4, 5]},\n",
       "             scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                                       (&#x27;logre2&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logre2__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logre2__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;logre2__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logre2__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;,\n",
       "                                            &#x27;sag&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;pca__n_components&#x27;: [2, 3, 4],\n",
       "                         &#x27;selectkbest__k&#x27;: [3, 4, 5]},\n",
       "             scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA()),\n",
       "                (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                (&#x27;logre2&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('pca', PCA()),\n",
       "                                       ('selectkbest', SelectKBest()),\n",
       "                                       ('logre2', LogisticRegression())]),\n",
       "             param_grid={'logre2__C': [0.1, 1, 10],\n",
       "                         'logre2__class_weight': [None, 'balanced'],\n",
       "                         'logre2__penalty': ['l1', 'l2'],\n",
       "                         'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear',\n",
       "                                            'sag', 'saga'],\n",
       "                         'pca__n_components': [2, 3, 4],\n",
       "                         'selectkbest__k': [3, 4, 5]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search_pca = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search_pca.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_pca = grid_search.best_params_\n",
    "\n",
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_pca = grid_search_pca.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84     58367\n",
      "           1       0.21      0.71      0.33      5592\n",
      "\n",
      "    accuracy                           0.75     63959\n",
      "   macro avg       0.59      0.73      0.59     63959\n",
      "weighted avg       0.90      0.75      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_pca = grid_search_pca.predict(X_test)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo_pca = y_pred_pca\n",
    "with open('../models/trained_model_PCA.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_pca, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDES NEURONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)['val_recall'].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
