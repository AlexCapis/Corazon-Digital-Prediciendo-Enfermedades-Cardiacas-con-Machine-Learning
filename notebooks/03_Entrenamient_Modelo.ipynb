{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"Explorando el corazón: Utilizando datos clínicos y de estilo de vida para predecir enfermedades cardíacas\"\n",
    "\n",
    "## ENTRENAMIENTO DE LOS MODELOS\n",
    "\n",
    "A lo largo de este notebook, abordaremos el problema de clasificación y nos enfocaremos en entrenar varios modelos con el objetivo de obtener las métricas más óptimas para nuestro problema. Nuestro objetivo final es encontrar el modelo que brinde el mejor rendimiento en la clasificación de los datos. A través de este proceso de entrenamiento y evaluación, exploraremos diferentes algoritmos y técnicas para optimizar nuestros modelos y lograr resultados confiables y eficientes. Estaremos atentos a métricas como la precisión, la exactitud, el puntaje F1 y otras medidas relevantes para evaluar el desempeño de cada modelo. Al final de este notebook, esperamos haber identificado el modelo más adecuado que se ajuste a nuestras necesidades y nos proporcione las mejores métricas en nuestra tarea de clasificación.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Estrategias para el balanceo del problema</summary>\n",
    "<p>\n",
    "En primer lugar, trataremos de aplicar distintas estrategias para poder solucionar el problema del desbalanceo del problema, aplicando técnicas como:\n",
    "\n",
    "    1. Penalización para compensar\n",
    "\n",
    "    2. Oversampling de la clase minoritaria\n",
    "\n",
    "    3. Combinación de resampling con Smote-Tomek\n",
    "    \n",
    "    4. Ensamble de Modelos con Balanceo\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Modelos utilizados</summary>\n",
    "<p>\n",
    "En segundo lugar, a lo largo de este notebook aplicaremos distintos modelos para encontrar las mejores métricas posibles para nuestro problema como por ejemplo:\n",
    "\n",
    "1. Aprendizaje supervisado\n",
    "    * **Logistic Regresion**\n",
    "    * **Naive Bayes**\n",
    "    * **Decision Tree Classifier**\n",
    "    * **Random Forest Classifier**\n",
    "    * **XGB Classifier**\n",
    "\n",
    "\n",
    "2. Aprendizaje no supervisado:\n",
    "    * **PCA**\n",
    "</p>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las distintas librerias necesarias para el análisis\n",
    "\n",
    "# Tratamiento de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Modelado de datos\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Tratamiento de datos desbalanceados\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from collections import Counter\n",
    "from pylab import rcParams\n",
    "\n",
    "# ímportamos los modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Optimización de Modelos\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Para guardar los modelos\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory_encoded</th>\n",
       "      <th>Diabetic_encoded</th>\n",
       "      <th>BMI_Category_Ordinal</th>\n",
       "      <th>GrupoSalud_Ordinal</th>\n",
       "      <th>GrupoSalud_Mental_Ordinal</th>\n",
       "      <th>SleepGroup_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease  Smoking  Stroke  DiffWalking  Sex  AgeCategory_encoded   \n",
       "0             0        1       0            0    0                    7  \\\n",
       "1             0        0       1            0    0                   12   \n",
       "2             0        1       0            0    1                    9   \n",
       "3             0        0       0            0    0                   11   \n",
       "4             0        0       0            1    0                    4   \n",
       "\n",
       "   Diabetic_encoded  BMI_Category_Ordinal  GrupoSalud_Ordinal   \n",
       "0                 2                     1                   1  \\\n",
       "1                 0                     2                   1   \n",
       "2                 2                     3                   3   \n",
       "3                 0                     2                   1   \n",
       "4                 0                     2                   3   \n",
       "\n",
       "   GrupoSalud_Mental_Ordinal  SleepGroup_Ordinal  \n",
       "0                          3                   1  \n",
       "1                          1                   3  \n",
       "2                          3                   3  \n",
       "3                          1                   2  \n",
       "4                          1                   3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el archivo de data/preprocessed_heart\n",
    "df= pd.read_csv(\"../data/processed/processed_heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HeartDisease', 'Smoking', 'Stroke', 'DiffWalking', 'Sex',\n",
       "       'AgeCategory_encoded', 'Diabetic_encoded', 'BMI_Category_Ordinal',\n",
       "       'GrupoSalud_Ordinal', 'GrupoSalud_Mental_Ordinal',\n",
       "       'SleepGroup_Ordinal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease           100.000000\n",
       "AgeCategory_encoded     23.343224\n",
       "DiffWalking             20.125805\n",
       "Stroke                  19.683530\n",
       "Diabetic_encoded        16.855285\n",
       "GrupoSalud_Ordinal      16.731876\n",
       "Smoking                 10.776416\n",
       "Sex                      7.004048\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para observar mejor la correlación que se produce con respecto a la varible target de \"HeartDisease\"\n",
    "# Se muestra en porcentage % y por orden ascendente\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix['HeartDisease'].sort_values(ascending=False) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease           2.962525\n",
       "Smoking                0.355585\n",
       "Stroke                 4.851460\n",
       "DiffWalking            2.088606\n",
       "Sex                    0.099029\n",
       "AgeCategory_encoded   -0.263611\n",
       "Diabetic_encoded       2.088800\n",
       "GrupoSalud_Ordinal     2.199359\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos skew para calcular la asimetría de nuestro DataFrame\n",
    "    # Un valor positivo indica una cola más larga hacia la derecha, lo que significa que hay valores extremos más altos. \n",
    "    # Un valor negativo indica una cola más larga hacia la izquierda, lo que significa que hay valores extremos más bajos.\n",
    "df.skew(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease\n",
      "0    292422\n",
      "1     27373\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Smoking\n",
      "0    187887\n",
      "1    131908\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Stroke\n",
      "0    307726\n",
      "1     12069\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DiffWalking\n",
      "0    275385\n",
      "1     44410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sex\n",
      "0    167805\n",
      "1    151990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AgeCategory_encoded\n",
      "9     34151\n",
      "8     33686\n",
      "10    31065\n",
      "7     29757\n",
      "6     25382\n",
      "12    24153\n",
      "5     21791\n",
      "11    21482\n",
      "0     21064\n",
      "4     21006\n",
      "3     20550\n",
      "2     18753\n",
      "1     16955\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Diabetic_encoded\n",
      "0    269653\n",
      "2     40802\n",
      "1      6781\n",
      "3      2559\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GrupoSalud_Ordinal\n",
      "1    265043\n",
      "2     28748\n",
      "3     26004\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hacemos un bucle para comprobar todas las columnas cuales están desbalanceadas y cuales no\n",
    "for column in df.columns:\n",
    "    print(df[column].value_counts(sort=True))  # Contamos los valores de la columna y mostramos los resultados ordenados\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop =[\"GrupoSalud_Mental_Ordinal\", \"SleepGroup_Ordinal\", \"BMI_Category_Ordinal\"]\n",
    "\n",
    "df= df.drop(columns_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestras etiquetas y features\n",
    "y = df['HeartDisease']\n",
    "X = df.drop('HeartDisease', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Smoking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory_encoded',\n",
       "       'Diabetic_encoded', 'GrupoSalud_Ordinal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (255836, 7)\n",
      "X_test (63959, 7)\n",
      "y_train (255836,)\n",
      "y_test (63959,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos el modelo sin aplicar técnicas para desbalanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos las características \n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42, solver='newton-cg')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos el modelo de Regresión Logística\n",
    "logre = LogisticRegression(C=1.0,penalty='l2',random_state=42,solver=\"newton-cg\")\n",
    "logre.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Sin_Balanceo = logre.predict(X_test_scaled) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.9125689895089042\n",
      "Precisión por clase: 0.5\n",
      "Recall por clase: 0.08369098712446352\n",
      "Puntuación F1 por clase: 0.14338235294117646\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculamos las metricas\n",
    "accuracy = accuracy_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Precisión del modelo:\", accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Precisión por clase:\", precision)\n",
    "\n",
    "recall = recall_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Recall por clase:\", recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Puntuación F1 por clase:\", f1)\n",
    "\n",
    "report = classification_report(y_test, y_pred_Sin_Balanceo)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "**Precisión del modelo**: 0.9137785467849363. Esta métrica representa la proporción de predicciones correctas sobre el total de predicciones realizadas. En este caso, el modelo tiene una precisión del 91.38%, lo que significa que aproximadamente el 91.38% de las predicciones son correctas.\n",
    "\n",
    "**Precisión por clase**: 0.5062068965517241. Esta métrica representa la proporción de predicciones positivas que son correctas para la clase \"1\" (enfermedad cardíaca) en relación con todas las predicciones positivas realizadas para esa clase. Aquí, la precisión para la clase \"1\" es del 50.62%, lo que indica que alrededor del 50.62% de las predicciones positivas para la enfermedad cardíaca son correctas.\n",
    "\n",
    "**Recall por clase**: 0.08854041013268998. Esta métrica, también conocida como sensibilidad o tasa de verdaderos positivos, representa la proporción de casos positivos (enfermedad cardíaca) que son correctamente identificados por el modelo. En este caso, el recall para la clase \"1\" es del 8.85%, lo que significa que el modelo identifica correctamente aproximadamente el 8.85% de los casos reales de enfermedad cardíaca.\n",
    "\n",
    "**Puntuación F1 por clase**: 0.15071868583162218. La puntuación F1 es una medida que combina la precisión y el recall en una única métrica. Es útil cuando hay un desequilibrio entre las clases, como en este caso. La puntuación F1 para la clase \"1\" es del 15.07%, lo que indica un equilibrio entre la precisión y el recall para esa clase.\n",
    "\n",
    "En cuanto al classification report, proporciona un resumen detallado de las métricas por clase y para el conjunto de datos completo. Observamos que la clase \"0\" (no enfermedad cardíaca) tiene una precisión alta, recall alto y puntuación F1 alta, mientras que la clase \"1\" (enfermedad cardíaca) muestra valores más bajos en todas las métricas. Esto puede indicar un desequilibrio en los datos, donde la clase \"1\" tiene menos representación en comparación con la clase \"0\". El conjunto de datos está sesgado hacia la clase \"0\" y eso puede afectar el rendimiento del modelo en la detección de la clase minoritaria (enfermedad cardíaca).\n",
    "\n",
    "En resumen, las métricas indican que el modelo tiene una precisión general alta, pero tiene dificultades para identificar correctamente los casos de enfermedad cardíaca. Esto se debe al desequilibrio en los datos y es importante tenerlo en cuenta al interpretar los resultados.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[57899   468]\n",
      " [ 5124   468]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusión \n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred_Sin_Balanceo) \n",
    "print(\"Matriz de confusión:\") \n",
    "print(confusion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "modelo_SinBalanceo = y_pred_Sin_Balanceo\n",
    "with open('../models/trained_model_Sin_Balanceo_Reg_Log.pkl', 'wb') as file:\n",
    "    pickle.dump(modelo_SinBalanceo, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APLICACIÓN DE LAS DISTINTAS TÉCNICAS PARA DESBALANCEAR EL PROBLEMA\n",
    "En esta sección, aplicaremos diferentes técnicas de desbalanceo de datos utilizando el mismo modelo con el objetivo de determinar la estrategia más efectiva. Evaluaremos las siguientes estrategias:\n",
    "\n",
    "<details>\n",
    "<summary>Técnicas utilizadas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "**Penalización para compensar**: Esta estrategia implica la aplicación de penalizaciones en el algoritmo de aprendizaje automático para compensar el desequilibrio en los datos. Se le asigna un peso mayor a las instancias de la clase minoritaria durante el entrenamiento para asegurar que se les dé más importancia en la predicción final.\n",
    "\n",
    "**Oversampling de la clase minoritaria**: En esta estrategia, generamos nuevas muestras sintéticas de la clase minoritaria para igualar la cantidad de muestras con la clase mayoritaria. Esto se logra replicando o generando nuevas instancias de la clase minoritaria en el conjunto de datos.\n",
    "\n",
    "**Combinación de resampling con Smote-Tomek:** Esta estrategia combina el undersampling y el oversampling para abordar el desbalanceo. El undersampling se aplica para reducir la cantidad de muestras de la clase mayoritaria, mientras que el oversampling se utiliza para generar nuevas muestras sintéticas de la clase minoritaria. Smote-Tomek es una técnica específica que combina el algoritmo SMOTE (Synthetic Minority Over-sampling Technique) con el método Tomek, que elimina instancias cercanas entre las dos clases.\n",
    "\n",
    "**Ensamble de Modelos con Balanceo**: En esta estrategia, utilizamos un ensamble de modelos para abordar el desbalanceo. Se entrenan varios modelos de aprendizaje automático utilizando diferentes técnicas de balanceo de datos. Luego, se combinan las predicciones de estos modelos para obtener una predicción final más robusta y equilibrada.\n",
    "\n",
    "Al evaluar estas estrategias y comparar su desempeño, podremos determinar cuál es la más adecuada para abordar el desbalanceo en nuestros datos y mejorar la calidad de nuestro modelo de aprendizaje automático.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Penalización para compensar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos un parámetro adicional en el modelo de Regresión logística en donde indicamos **weight = “balanced”** y con esto el algoritmo se encargará de equilibrar a la clase minoritaria durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# EscalaMOS las características \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos \n",
    "\n",
    "# Modelo de Logistic Regression\n",
    "logre2 = LogisticRegression(C=0.1,penalty='l1',random_state=42,solver=\"liblinear\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "logre2.fit(X_train_scaled, y_train)\n",
    "y_pred2 = logre2.predict(X_test_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42480 15887]\n",
      " [ 1280  4312]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion2 = confusion_matrix(y_test, y_pred2)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "En comparación con el modelo anterior, el nuevo modelo de Logistic Regression con los datos de prueba actualizados muestra una mejora en ciertas métricas, mientras que otras disminuyen. Aquí hay un resumen de las diferencias clave:\n",
    "\n",
    "**Precisión general**: El nuevo modelo tiene una precisión general de 0.74, lo que indica que aproximadamente el 74% de las predicciones son correctas. Esta precisión es menor que la del modelo anterior, que era de 0.91.\n",
    "\n",
    "**Precisión por clase**: Para la clase \"0\" (no enfermedad cardíaca), la precisión es alta con un valor de 0.97, lo que indica que la mayoría de las predicciones negativas son correctas. Sin embargo, para la clase \"1\" (enfermedad cardíaca), la precisión es baja con un valor de 0.22, lo que indica que la mayoría de las predicciones positivas son incorrectas.\n",
    "\n",
    "**Recall por clase**: El nuevo modelo muestra un recall alto para la clase \"1\" (enfermedad cardíaca) con un valor de 0.77, lo que indica que la mayoría de los casos reales de enfermedad cardíaca son correctamente identificados por el modelo. Sin embargo, el recall para la clase \"0\" (no enfermedad cardíaca) es de 0.74, lo que indica que hay algunos casos reales de no enfermedad cardíaca que no son identificados por el modelo.\n",
    "\n",
    "**Puntuación F1 por clase**: La puntuación F1 para la clase \"1\" (enfermedad cardíaca) es de 0.34, lo que indica un equilibrio entre la precisión y el recall para esa clase. Sin embargo, la puntuación F1 para la clase \"0\" (no enfermedad cardíaca) es de 0.84, lo que indica un mejor equilibrio entre precisión y recall para esa clase.\n",
    "\n",
    "En general, el nuevo modelo muestra una mejora en la identificación de casos de enfermedad cardíaca, como se refleja en un recall más alto para la clase \"1\". Sin embargo, la precisión para la clase \"1\" ha disminuido, lo que significa que hay un mayor número de falsos positivos en comparación con el modelo anterior. Además, la precisión general y la puntuación F1 han disminuido.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Oversampling de la clase minoritaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, crearemos nuevas **muestras “sintéticas” de la clase minoritaria**. Usando RandomOverSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 117027})\n"
     ]
    }
   ],
   "source": [
    "# Escalamos los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Aplicamos el RandomOverSampler\n",
    "os = RandomOverSampler(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Imprimimos la diferencia del antes y después del resampling\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Definimos y entrenamos el modelo en este caso de Regresión Logistica 8para mostrar la diferencia con respecto al no desbalanceado)\n",
    "model = LogisticRegression(C=0.1,penalty='l2',random_state=42,solver=\"newton-cg\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "pred_y_random = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.74      0.84     58367\n",
      "           1       0.22      0.75      0.34      5592\n",
      "\n",
      "    accuracy                           0.74     63959\n",
      "   macro avg       0.59      0.75      0.59     63959\n",
      "weighted avg       0.90      0.74      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el classification report\n",
    "report_RandomOverSampler = classification_report(y_test, pred_y_random)\n",
    "print(\"Classification Report:\")\n",
    "print(report_RandomOverSampler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "La **precisión global (accuracy)** es del 74%, lo que significa que el 74% se clasificaron correctamente.\n",
    "\n",
    "**La precisión** para la clase 0 es del 97% y para la clase 1 es del 22%. Esto indica que, con RandomOverSampler, el modelo tiene una precisión más equilibrada entre ambas clases.\n",
    "\n",
    "**El recall** para la clase 0 es del 74%, lo que indica que se detectaron correctamente el 74% de los ejemplos de la clase 0. Para la clase 1, el recall es del 75%, lo que indica que se detectaron correctamente el 75% de los ejemplos de la clase 1.\n",
    "\n",
    "**El f1-score** para la clase 0 es del 0.84, indicando un equilibrio razonable entre precisión y recall. Para la clase 1, el f1-score es del 0.34, lo que indica un desequilibrio entre precisión y recall.\n",
    "\n",
    ".\n",
    "\n",
    "En resumen, al aplicar RandomOverSampler para abordar el desbalanceo de clases, se obtiene un mayor equilibrio entre las métricas de precisión y recall para ambas clases. Sin embargo, es importante tener en cuenta que el f1-score y la precisión global (accuracy) siguen siendo más altos en el caso de no aplicar el desbalanceo, aunque con un desequilibrio significativo en las métricas de la clase minoritaria (1).\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[43225 15142]\n",
      " [ 1376  4216]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion_RandomOverSampler = confusion_matrix(y_test, pred_y_random)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_RandomOverSampler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Combinamos resampling con Smote-Tomek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probaremos una técnica muy usada que consiste en aplicar en simultáneo un algoritmo de subsampling y otro de oversampling a la vez al dataset. \n",
    "\n",
    "    En este caso usaremos SMOTE para oversampling: busca puntos vecinos cercanos y agrega puntos “en linea recta” entre ellos. \n",
    "\n",
    "    Y usaremos Tomek para undersampling que quita los de distinta clase que sean nearest neighbor y deja ver mejor el decisión boundary (la zona limítrofe de nuestras clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 234054})\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el SMOTE para oversampling\n",
    "smote = SMOTE()\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Aplicamos Tomek Links para undersampling\n",
    "tomek = TomekLinks()\n",
    "X_train_res, y_train_res = tomek.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Imprimimos la diferencia del antes y después del resampling\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Definimos y entrenamos el modelo en este caso de Regresión Logistica 8para mostrar la diferencia con respecto al no desbalanceado)\n",
    "model = LogisticRegression(C=1.0, penalty='l2', random_state=42, solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "pred_y_Smote_Tomek = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report_Smote_Tomet = classification_report(y_test, pred_y_Smote_Tomek)\n",
    "print(\"Classification Report:\")\n",
    "print(report_Smote_Tomet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "**La precisión global (accuracy)** es del 73%, lo que significa que el 73% se clasificaron correctamente.\n",
    "\n",
    "**La precisión** para la clase 0 es del 97% y para la clase 1 es del 21%. Esto indica que, con la aplicación de SMOTE y TomekLinks, el modelo tiene una precisión más equilibrada entre ambas clases, aunque la precisión para la clase 1 sigue siendo baja.\n",
    "\n",
    "**El recall** para la clase 0 es del 73%, lo que indica que se detectaron correctamente el 73% de los ejemplos de la clase 0. Para la clase 1, el recall es del 77%, lo que indica que se detectaron correctamente el 77% de los ejemplos de la clase 1.\n",
    "\n",
    "**El f1-score** para la clase 0 es del 0.83, indicando un equilibrio razonable entre precisión y recall. Para la clase 1, el f1-score es del 0.33, lo que indica un desequilibrio entre precisión y recall.\n",
    "\n",
    "En resumen, al utilizar la técnica de SMOTE y TomekLinks para abordar el desbalanceo de clases, se obtiene un mayor equilibrio entre las métricas de precisión y recall para ambas clases en comparación con el caso sin aplicar el desbalanceo. Sin embargo, sigue existiendo un desequilibrio en las métricas de la clase minoritaria (1), con una precisión y f1-score bajos. \n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42441 15926]\n",
      " [ 1274  4318]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion_Smote_Tomet = confusion_matrix(y_test, pred_y_Smote_Tomek)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_Smote_Tomet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Ensamble de Modelos con Balanceo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estrategia usaremos un Clasificador de Ensamble que utiliza Bagging y el modelo será un DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=True,\n",
    "                                random_state=0,)\n",
    " \n",
    "# Lo entrenamos\n",
    "bbc.fit(X_train, y_train)\n",
    "pred_y_Bagging = bbc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el classification report\n",
    "report_Bagging = classification_report(y_test, pred_y_Bagging)\n",
    "print(\"Classification Report:\")\n",
    "print(report_Bagging)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "**La precisión global (accuracy)** es del 71%, lo que significa que el 71%  se clasificaron correctamente.\n",
    "\n",
    "**La precisión** para la clase 0 es del 97% y para la clase 1 es del 20%. Esto indica que, con la aplicación de BalancedBaggingClassifier, el modelo tiene una precisión más equilibrada entre ambas clases, aunque la precisión para la clase 1 sigue siendo baja.\n",
    "\n",
    "**El recall** para la clase 0 es del 70%, lo que indica que se detectaron correctamente el 70% de los ejemplos de la clase 0. Para la clase 1, el recall es del 80%, lo que indica que se detectaron correctamente el 80% de los ejemplos de la clase 1.\n",
    "\n",
    "**El f1-score** para la clase 0 es del 0.81, indicando un equilibrio razonable entre precisión y recall. Para la clase 1, el f1-score es del 0.32, lo que indica un desequilibrio entre precisión y recall.\n",
    "\n",
    "En resumen, al utilizar la técnica de BalancedBaggingClassifier para abordar el desbalanceo de clases, se obtiene un mayor equilibrio entre las métricas de precisión y recall para ambas clases en comparación con el caso sin aplicar el desbalanceo. Sin embargo, sigue existiendo un desequilibrio en las métricas de la clase minoritaria (1), con una precisión y f1-score bajos.\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[40827 17540]\n",
      " [ 1118  4474]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion_Bagging = confusion_matrix(y_test, pred_y_Bagging)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_Bagging)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELOS A TRATAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELOS SUPERVISADOS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOGISTIC REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline con escalamiento y modelo Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'logre2__C': [0.1, 1, 10], # regularización\n",
    "    'logre2__penalty': ['l1', 'l2'], # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # se prueba cada algoritmo\n",
    "    'logre2__class_weight': [None, 'balanced'],  # sin balanceo y con balanceo\n",
    "    'selectkbest__k': [5, 6, 7] # se seleccionan los mejores k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall') # He probado con varios cv y el cv=5 es el que mejor se adapta\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_logRe = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.69      0.81     58367\n",
      "           1       0.19      0.77      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.73      0.56     63959\n",
      "weighted avg       0.90      0.70      0.76     63959\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "{'logre2__C': 0.1, 'logre2__class_weight': 'balanced', 'logre2__penalty': 'l1', 'logre2__solver': 'liblinear', 'selectkbest__k': 5}\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el classification report\n",
    "report2 = classification_report(y_test, y_pred_logRe)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params_logRe = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params_logRe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "mejor_modelo_logRe = y_pred_logRe\n",
    "with open('../models/trained_model_Log_Regression.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_logRe, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el BalancedBaggingClassifier para desbalancear los datos (mismo que antes)\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=True,\n",
    "                                random_state=0)\n",
    "\n",
    "bbc.fit(X_train, y_train)\n",
    "pred_yBagging = bbc.predict(X_test)\n",
    "\n",
    "# Creamos el pipeline con SelectKBest y Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('naive_bayes', GaussianNB())\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'selectkbest__k': [7, 8, 9],\n",
    "    'naive_bayes__var_smoothing': [1e-9, 1e-10, 1e-11]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=10, scoring= \"recall\")\n",
    "\n",
    "# Entrenamos el GridSearchCV en los datos desbalanceados\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos la mejor combinación de parámetros\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_Naive = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     58367\n",
      "           1       0.29      0.38      0.33      5592\n",
      "\n",
      "    accuracy                           0.86     63959\n",
      "   macro avg       0.61      0.65      0.63     63959\n",
      "weighted avg       0.88      0.86      0.87     63959\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "{'naive_bayes__var_smoothing': 1e-09, 'selectkbest__k': 7}\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el classification report\n",
    "report2 = classification_report(y_test, y_pred_Naive)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)\n",
    "\n",
    "\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "mejor_modelo_Naive = y_pred_Naive\n",
    "with open('../models/trained_model_NaiveBayes.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_Naive, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Support Vector Classification será el próximo modelo a entrenar, el cual no ha prodido realizarse en esta fase temprana debido a su complejidad en cuanto a la capidad temperol se refiere, ya que es un modelo qeu requiere de un amplio tiempo para poder desarrollarse y en este caso tras varios intentos no ha sido posible estar entrenando dicho modelo tantas horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Creamos el pipeline con escalamiento, selección de características y modelo SVC\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# Definimos  los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "    'svc__class_weight': [None, 'balanced'],\n",
    "    'selectkbest__k': [5, 6, 7]\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\\ngrid_search_SVC = GridSearchCV(pipeline, parameters, cv=2, scoring='recall')\\ngrid_search_SVC.fit(X_train, y_train)\\n\\n# Obtenemos las mejores configuraciones de hiperparámetros encontradas\\nbest_params = grid_search_SVC.best_params_\\nprint(best_params)\\n# Obtenemos las predicciones del mejor modelo encontrado\\ny_pred_SVC = grid_search_SVC.predict(X_test)\\n\\nprint(classification_report(y_test, y_pred_SVC))\\n\\nmejor_modelo = y_pred_SVC\\n\\n# Guardamos el modelo\\nwith open('../models/trained_model_SVC.pkl', 'wb') as file:\\n    pickle.dump(mejor_modelo, file) \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search_SVC = GridSearchCV(pipeline, parameters, cv=2, scoring='recall')\n",
    "grid_search_SVC.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params = grid_search_SVC.best_params_\n",
    "print(best_params)\n",
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_SVC = grid_search_SVC.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_SVC))\n",
    "\n",
    "mejor_modelo = y_pred_SVC\n",
    "\n",
    "# Guardamos el modelo\n",
    "with open('../models/trained_model_SVC.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo, file) \n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISSION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline con DecisionTreeClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('dt', DecisionTreeClassifier(random_state=42, class_weight='balanced')) # Para solucionar el problema de balanceo class_weight='balanced'\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'dt__max_depth': [5, 8, 10],\n",
    "    'dt__min_samples_split': [2, 5, 10],\n",
    "    'dt__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params = grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     58367\n",
      "           1       0.18      0.81      0.30      5592\n",
      "\n",
      "    accuracy                           0.66     63959\n",
      "   macro avg       0.58      0.73      0.54     63959\n",
      "weighted avg       0.90      0.66      0.74     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo con los mejores hiperparámetros\n",
    "pipeline.set_params(**best_params) # ** se utiliza para desempaquetar un diccionario y pasar sus elementos como argumentos de palabras clave a una función o método\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las predicciones\n",
    "y_pred_decisiontree = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_decisiontree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo = y_pred_decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "with open('../models/trained_model_decisionTree.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={&#x27;rfc__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                                        &#x27;rfc__max_depth&#x27;: [5, 8, 10],\n",
       "                                        &#x27;rfc__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [50, 80, 100]},\n",
       "                   random_state=0, scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={&#x27;rfc__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n",
       "                                        &#x27;rfc__max_depth&#x27;: [5, 8, 10],\n",
       "                                        &#x27;rfc__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [50, 80, 100]},\n",
       "                   random_state=0, scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rfc&#x27;, RandomForestClassifier(random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('rfc',\n",
       "                                              RandomForestClassifier(random_state=0))]),\n",
       "                   param_distributions={'rfc__class_weight': ['balanced', None],\n",
       "                                        'rfc__max_depth': [5, 8, 10],\n",
       "                                        'rfc__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rfc__min_samples_split': [2, 5, 10],\n",
       "                                        'rfc__n_estimators': [50, 80, 100]},\n",
       "                   random_state=0, scoring='recall')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el pipeline con Random Forest Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('rfc', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el RandomizedSearchCV\n",
    "parameters = {\n",
    "    'rfc__n_estimators': [50, 80, 100],\n",
    "    'rfc__max_depth': [5, 8, 10],\n",
    "    'rfc__min_samples_split': [2, 5, 10],\n",
    "    'rfc__min_samples_leaf': [1, 2, 4],\n",
    "    'rfc__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Realizamos la búsqueda aleatoria de hiperparámetros utilizando RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, parameters, cv=5, scoring='recall', random_state=0)\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__n_estimators': 100,\n",
       " 'rfc__min_samples_split': 10,\n",
       " 'rfc__min_samples_leaf': 2,\n",
       " 'rfc__max_depth': 8,\n",
       " 'rfc__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_RandomForest = random_search.best_params_\n",
    "best_params_RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_RandomForest = random_search.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_RandomForest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "mejor_modelo_RandomForest = y_pred_RandomForest\n",
    "\n",
    "with open('../models/trained_model_RandomForest.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_RandomForest, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBCCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline con escalado y XGBoost Classifier con scale_pos_weight\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=0, scale_pos_weight=3)) # scale_pos_weight=3 para el balanceo\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'xgb__max_depth': [2, 3, 4],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;xgb&#x27;,\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={&#x27;xgb__learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;xgb__max_depth&#x27;: [2, 3, 4]},\n",
       "             scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;xgb&#x27;,\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={&#x27;xgb__learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;xgb__max_depth&#x27;: [2, 3, 4]},\n",
       "             scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;xgb&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=0, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('xgb',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      feature_types=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=No...\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_threshold=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0, ...))]),\n",
       "             param_grid={'xgb__learning_rate': [0.1, 0.01, 0.001],\n",
       "                         'xgb__max_depth': [2, 3, 4]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search_XGB = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgb__learning_rate': 0.1, 'xgb__max_depth': 4}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_XGB = grid_search_XGB.best_params_\n",
    "best_params_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     58367\n",
      "           1       0.36      0.33      0.34      5592\n",
      "\n",
      "    accuracy                           0.89     63959\n",
      "   macro avg       0.65      0.64      0.64     63959\n",
      "weighted avg       0.89      0.89      0.89     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_XGB = grid_search_XGB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos\n",
    "mejor_modelo_XGB = y_pred_XGB\n",
    "with open('../models/trained_model_XGB_Classifiert.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_XGB, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO NO SUPERVISADO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline \n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definimos los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'pca__n_components': [2, 3, 4],  # Número de componentes para PCA\n",
    "    'logre2__C': [0.1, 1, 10],  # Regularización\n",
    "    'logre2__penalty': ['l1', 'l2'],  # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # Algoritmos de optimización\n",
    "    'logre2__class_weight': [None, 'balanced'],  # Sin balanceo y con balanceo\n",
    "    'selectkbest__k': [3, 4, 5]  # Seleccionar los mejores k\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                                       (&#x27;logre2&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logre2__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logre2__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;logre2__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logre2__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;,\n",
       "                                            &#x27;sag&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;pca__n_components&#x27;: [2, 3, 4],\n",
       "                         &#x27;selectkbest__k&#x27;: [3, 4, 5]},\n",
       "             scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                                       (&#x27;logre2&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;logre2__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logre2__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;logre2__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logre2__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;,\n",
       "                                            &#x27;sag&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;pca__n_components&#x27;: [2, 3, 4],\n",
       "                         &#x27;selectkbest__k&#x27;: [3, 4, 5]},\n",
       "             scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA()),\n",
       "                (&#x27;selectkbest&#x27;, SelectKBest()),\n",
       "                (&#x27;logre2&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('pca', PCA()),\n",
       "                                       ('selectkbest', SelectKBest()),\n",
       "                                       ('logre2', LogisticRegression())]),\n",
       "             param_grid={'logre2__C': [0.1, 1, 10],\n",
       "                         'logre2__class_weight': [None, 'balanced'],\n",
       "                         'logre2__penalty': ['l1', 'l2'],\n",
       "                         'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear',\n",
       "                                            'sag', 'saga'],\n",
       "                         'pca__n_components': [2, 3, 4],\n",
       "                         'selectkbest__k': [3, 4, 5]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizamos la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search_pca = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search_pca.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las mejores configuraciones de hiperparámetros encontradas\n",
    "best_params_pca = grid_search.best_params_\n",
    "\n",
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_pca = grid_search_pca.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84     58367\n",
      "           1       0.21      0.71      0.33      5592\n",
      "\n",
      "    accuracy                           0.75     63959\n",
      "   macro avg       0.59      0.73      0.59     63959\n",
      "weighted avg       0.90      0.75      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos las predicciones del mejor modelo encontrado\n",
    "y_pred_pca = grid_search_pca.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "mejor_modelo_pca = y_pred_pca\n",
    "with open('../models/trained_model_PCA.pkl', 'wb') as file:\n",
    "    pickle.dump(mejor_modelo_pca, file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLICACIÓN DE LOS RESULTADOS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory_encoded</th>\n",
       "      <th>Diabetic_encoded</th>\n",
       "      <th>GrupoSalud_Ordinal</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63954</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63955</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63956</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63958</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63959 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smoking  Stroke  DiffWalking  Sex  AgeCategory_encoded   \n",
       "0            1       0            0    0                    1  \\\n",
       "1            0       0            0    0                    2   \n",
       "2            1       0            0    0                    4   \n",
       "3            0       0            0    0                    9   \n",
       "4            1       0            1    0                    8   \n",
       "...        ...     ...          ...  ...                  ...   \n",
       "63954        1       0            0    1                   12   \n",
       "63955        1       0            0    1                    3   \n",
       "63956        0       0            0    0                    3   \n",
       "63957        0       0            0    1                   10   \n",
       "63958        1       0            1    1                    8   \n",
       "\n",
       "       Diabetic_encoded  GrupoSalud_Ordinal  HeartDisease  \n",
       "0                     0                   1             0  \n",
       "1                     0                   1             0  \n",
       "2                     0                   1             0  \n",
       "3                     0                   3             0  \n",
       "4                     0                   3             0  \n",
       "...                 ...                 ...           ...  \n",
       "63954                 0                   1             0  \n",
       "63955                 0                   2             0  \n",
       "63956                 0                   1             0  \n",
       "63957                 0                   1             0  \n",
       "63958                 2                   3             0  \n",
       "\n",
       "[63959 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el archivo de data/test\n",
    "y_test = pd.read_csv(\"../data/test/test.csv\")\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos únicamente la columna a predecir\n",
    "y_test = y_test['HeartDisease']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo trained_model_RandomForest\n",
    "with open('../models/trained_model_RandomForest.pkl', 'rb') as file:\n",
    "    mejor_modelo_RandomForest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_RandomForest)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo desde el archivo\n",
    "with open('../models/trained_model_decisionTree.pkl', 'rb') as file:\n",
    "    mejor_modelo_decisionTree = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     58367\n",
      "           1       0.18      0.81      0.30      5592\n",
      "\n",
      "    accuracy                           0.66     63959\n",
      "   macro avg       0.58      0.73      0.54     63959\n",
      "weighted avg       0.90      0.66      0.74     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_decisionTree)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo desde el archivo\n",
    "with open('../models/trained_model_Log_Regression.pkl', 'rb') as file:\n",
    "    mejor_modelo_Log_Regression = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.69      0.81     58367\n",
      "           1       0.19      0.77      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.73      0.56     63959\n",
      "weighted avg       0.90      0.70      0.76     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_Log_Regression)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo desde el archivo\n",
    "with open('../models/trained_model_NaiveBayes.pkl', 'rb') as file:\n",
    "    mejor_modelo_NaiveBayes = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     58367\n",
      "           1       0.29      0.38      0.33      5592\n",
      "\n",
      "    accuracy                           0.86     63959\n",
      "   macro avg       0.61      0.65      0.63     63959\n",
      "weighted avg       0.88      0.86      0.87     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_NaiveBayes)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo desde el archivo\n",
    "with open('../models/trained_model_XGB_Classifiert.pkl', 'rb') as file:\n",
    "    mejor_modelo_XGB_Classifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.95     58367\n",
      "           1       0.53      0.06      0.10      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.72      0.53      0.53     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_XGB_Classifier)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo desde el archivo\n",
    "with open('../models/trained_model_PCA.pkl', 'rb') as file:\n",
    "    mejor_modelo_PCA = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84     58367\n",
      "           1       0.21      0.71      0.33      5592\n",
      "\n",
      "    accuracy                           0.75     63959\n",
      "   macro avg       0.59      0.73      0.59     63959\n",
      "weighted avg       0.90      0.75      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, mejor_modelo_PCA)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLICACIÓN\n",
    "\n",
    "<details>\n",
    "<summary>Explicación de las métricas</summary>\n",
    "<p>\n",
    "\n",
    "En el problema que se está tratando de resolver, el objetivo principal es lograr una alta recall, ya que indica la capacidad del modelo para identificar correctamente la mayoría de los casos positivos. Esto es especialmente importante en la detección de enfermedades cardíacas, donde es fundamental identificar adecuadamente a los pacientes que realmente padecen dicha enfermedad.\n",
    "\n",
    "Si nos enfocamos en el mayor recall posible, podemos observar que los modelos de RandomForest, Decision Tree y Regresión Logística muestran valores más altos de recall para la clase \"1\". Estos modelos tienen la capacidad de identificar correctamente la mayoría de los casos positivos.\n",
    "\n",
    "En contraste, los modelos de Naive Bayes y XGB Classifier presentan recalls más bajos para la clase \"1\", lo que indica que podrían no ser tan efectivos en la detección de casos positivos.\n",
    "\n",
    "Considerando todas las métricas y teniendo en cuenta tu objetivo principal de maximizar el recall, el modelo de RandomForest podría ser la mejor opción, ya que tiene unas métricas en conjunto más sólidas que el resto.\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
