{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"Explorando el corazón: Utilizando datos clínicos y de estilo de vida para predecir enfermedades cardíacas\"\n",
    "\n",
    "#### ENTRENAMIENTO DE LOS MODELOS\n",
    "\n",
    "A lo largo de este notebook, abordaremos el problema de clasificación y nos enfocaremos en entrenar varios modelos con el objetivo de obtener las métricas más óptimas para nuestro problema. Nuestro objetivo final es encontrar el modelo que brinde el mejor rendimiento y precisión en la clasificación de los datos. A través de este proceso de entrenamiento y evaluación, exploraremos diferentes algoritmos y técnicas para optimizar nuestros modelos y lograr resultados confiables y eficientes. Estaremos atentos a métricas como la precisión, la exactitud, el puntaje F1 y otras medidas relevantes para evaluar el desempeño de cada modelo. Al final de este notebook, esperamos haber identificado el modelo más adecuado que se ajuste a nuestras necesidades y nos proporcione las mejores métricas en nuestra tarea de clasificación.\n",
    "1. Logistic Regresion\n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las distintas librerias necesarias para el análisis\n",
    "\n",
    "# Tratamiento de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,log_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "      <th>AgeCategory_encoded</th>\n",
       "      <th>Race_encoded</th>\n",
       "      <th>Diabetic_encoded</th>\n",
       "      <th>GenHealth_encoded</th>\n",
       "      <th>BMI_Category_Ordinal</th>\n",
       "      <th>GrupoSalud_Ordinal</th>\n",
       "      <th>GrupoSalud_Mental_Ordinal</th>\n",
       "      <th>SleepGroup_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease  Smoking  AlcoholDrinking  Stroke  DiffWalking  Sex   \n",
       "0             0        1                0       0            0    0  \\\n",
       "1             0        0                0       1            0    0   \n",
       "2             0        1                0       0            0    1   \n",
       "3             0        0                0       0            0    0   \n",
       "4             0        0                0       0            1    0   \n",
       "\n",
       "   PhysicalActivity  Asthma  KidneyDisease  SkinCancer  AgeCategory_encoded   \n",
       "0                 1       1              0           1                    7  \\\n",
       "1                 1       0              0           0                   12   \n",
       "2                 1       1              0           0                    9   \n",
       "3                 0       0              0           1                   11   \n",
       "4                 1       0              0           0                    4   \n",
       "\n",
       "   Race_encoded  Diabetic_encoded  GenHealth_encoded  BMI_Category_Ordinal   \n",
       "0             5                 2                  4                     1  \\\n",
       "1             5                 0                  4                     2   \n",
       "2             5                 2                  1                     3   \n",
       "3             5                 0                  2                     2   \n",
       "4             5                 0                  4                     2   \n",
       "\n",
       "   GrupoSalud_Ordinal  GrupoSalud_Mental_Ordinal  SleepGroup_Ordinal  \n",
       "0                   1                          3                   1  \n",
       "1                   1                          1                   3  \n",
       "2                   3                          3                   3  \n",
       "3                   1                          1                   2  \n",
       "4                   3                          1                   3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el archivo de data/preprocessed_heart\n",
    "df= pd.read_csv(\"../data/processed/processed_heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease                 100.000000\n",
       "AgeCategory_encoded           23.343224\n",
       "DiffWalking                   20.125805\n",
       "Stroke                        19.683530\n",
       "Diabetic_encoded              16.855285\n",
       "GrupoSalud_Ordinal            16.731876\n",
       "KidneyDisease                 14.519710\n",
       "Smoking                       10.776416\n",
       "SkinCancer                     9.331688\n",
       "Sex                            7.004048\n",
       "BMI_Category_Ordinal           5.342468\n",
       "Asthma                         4.144415\n",
       "Race_encoded                   3.485362\n",
       "GrupoSalud_Mental_Ordinal      2.130066\n",
       "GenHealth_encoded             -1.106186\n",
       "AlcoholDrinking               -3.207974\n",
       "SleepGroup_Ordinal            -3.456049\n",
       "PhysicalActivity             -10.002993\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para observar mejor la correlación que se produce con respecto a la varible target de \"HeartDisease\"\n",
    "# Se muestra en porcentage % y por orden ascendente\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix['HeartDisease'].sort_values(ascending=False) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease                 2.962525\n",
       "Smoking                      0.355585\n",
       "AlcoholDrinking              3.429019\n",
       "Stroke                       4.851460\n",
       "DiffWalking                  2.088606\n",
       "Sex                          0.099029\n",
       "PhysicalActivity            -1.319602\n",
       "Asthma                       2.148059\n",
       "KidneyDisease                4.918135\n",
       "SkinCancer                   2.797757\n",
       "AgeCategory_encoded         -0.263611\n",
       "Race_encoded                -1.923663\n",
       "Diabetic_encoded             2.088800\n",
       "GenHealth_encoded           -0.129635\n",
       "BMI_Category_Ordinal         0.659454\n",
       "GrupoSalud_Ordinal           2.199359\n",
       "GrupoSalud_Mental_Ordinal    1.808620\n",
       "SleepGroup_Ordinal          -1.391516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos skew para calcular la asimetría de nuestro DataFrame\n",
    "    # Un valor positivo indica una cola más larga hacia la derecha, lo que significa que hay valores extremos más altos. \n",
    "    # Un valor negativo indica una cola más larga hacia la izquierda, lo que significa que hay valores extremos más bajos.\n",
    "df.skew(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease\n",
      "0    292422\n",
      "1     27373\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Smoking\n",
      "0    187887\n",
      "1    131908\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AlcoholDrinking\n",
      "0    298018\n",
      "1     21777\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Stroke\n",
      "0    307726\n",
      "1     12069\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DiffWalking\n",
      "0    275385\n",
      "1     44410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sex\n",
      "0    167805\n",
      "1    151990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PhysicalActivity\n",
      "1    247957\n",
      "0     71838\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Asthma\n",
      "0    276923\n",
      "1     42872\n",
      "Name: count, dtype: int64\n",
      "\n",
      "KidneyDisease\n",
      "0    308016\n",
      "1     11779\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SkinCancer\n",
      "0    289976\n",
      "1     29819\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AgeCategory_encoded\n",
      "9     34151\n",
      "8     33686\n",
      "10    31065\n",
      "7     29757\n",
      "6     25382\n",
      "12    24153\n",
      "5     21791\n",
      "11    21482\n",
      "0     21064\n",
      "4     21006\n",
      "3     20550\n",
      "2     18753\n",
      "1     16955\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Race_encoded\n",
      "5    245212\n",
      "3     27446\n",
      "2     22939\n",
      "4     10928\n",
      "1      8068\n",
      "0      5202\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Diabetic_encoded\n",
      "0    269653\n",
      "2     40802\n",
      "1      6781\n",
      "3      2559\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GenHealth_encoded\n",
      "4    113858\n",
      "2     93129\n",
      "0     66842\n",
      "1     34677\n",
      "3     11289\n",
      "Name: count, dtype: int64\n",
      "\n",
      "BMI_Category_Ordinal\n",
      "3    113640\n",
      "2     88661\n",
      "4     67157\n",
      "5     27379\n",
      "6     17844\n",
      "1      5114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GrupoSalud_Ordinal\n",
      "1    265043\n",
      "2     28748\n",
      "3     26004\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GrupoSalud_Mental_Ordinal\n",
      "1    247032\n",
      "2     45891\n",
      "3     26872\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SleepGroup_Ordinal\n",
      "3    222809\n",
      "2     66721\n",
      "1     30265\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hacemos un bucle para comprobar todas las columnas cuales están desbalanceadas y cuales no\n",
    "for column in df.columns:\n",
    "    print(df[column].value_counts(sort=True))  # Contamos los valores de la columna y mostramos los resultados ordenados\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probamos el modelo sin aplicar el desbalanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=([\"PhysicalActivity\", \"SleepGroup_Ordinal\",\"AlcoholDrinking\", \"GenHealth_encoded\", \"GrupoSalud_Mental_Ordinal\",\n",
    "                       \"Race_encoded\",\"Asthma\", \"BMI_Category_Ordinal\", \"SkinCancer\", \"KidneyDisease\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos nuestras etiquetas y features\n",
    "y = df['HeartDisease']\n",
    "X = df.drop('HeartDisease', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar las características \n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42, solver='newton-cg')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logre = LogisticRegression(C=1.0,penalty='l2',random_state=42,solver=\"newton-cg\")\n",
    "logre.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logre.predict(X_test_scaled) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.9125689895089042\n",
      "Precisión por clase: 0.5\n",
      "Recall por clase: 0.08369098712446352\n",
      "Puntuación F1 por clase: 0.14338235294117646\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Suponiendo que ya has realizado todas las operaciones previas y tienes las variables adecuadas\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", accuracy)\n",
    "\n",
    "# Calcular la precisión por clase\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precisión por clase:\", precision)\n",
    "\n",
    "# Calcular el recall (sensibilidad) por clase\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall por clase:\", recall)\n",
    "\n",
    "# Calcular la puntuación F1 por clase\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Puntuación F1 por clase:\", f1)\n",
    "\n",
    "# Calcular el classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Smoking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory_encoded',\n",
       "       'Diabetic_encoded', 'GrupoSalud_Ordinal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease           100.000000\n",
       "AgeCategory_encoded     23.343224\n",
       "DiffWalking             20.125805\n",
       "Stroke                  19.683530\n",
       "Diabetic_encoded        16.855285\n",
       "GrupoSalud_Ordinal      16.731876\n",
       "Smoking                 10.776416\n",
       "Sex                      7.004048\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix['HeartDisease'].sort_values(ascending=False) *100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLICACIÓN DE LAS MÉTRICAS**\n",
    "\n",
    "Precisión del modelo: 0.9137785467849363. Esta métrica representa la proporción de predicciones correctas sobre el total de predicciones realizadas. En este caso, el modelo tiene una precisión del 91.38%, lo que significa que aproximadamente el 91.38% de las predicciones son correctas.\n",
    "\n",
    "Precisión por clase: 0.5062068965517241. Esta métrica representa la proporción de predicciones positivas que son correctas para la clase \"1\" (enfermedad cardíaca) en relación con todas las predicciones positivas realizadas para esa clase. Aquí, la precisión para la clase \"1\" es del 50.62%, lo que indica que alrededor del 50.62% de las predicciones positivas para la enfermedad cardíaca son correctas.\n",
    "\n",
    "Recall por clase: 0.08854041013268998. Esta métrica, también conocida como sensibilidad o tasa de verdaderos positivos, representa la proporción de casos positivos (enfermedad cardíaca) que son correctamente identificados por el modelo. En este caso, el recall para la clase \"1\" es del 8.85%, lo que significa que el modelo identifica correctamente aproximadamente el 8.85% de los casos reales de enfermedad cardíaca.\n",
    "\n",
    "Puntuación F1 por clase: 0.15071868583162218. La puntuación F1 es una medida que combina la precisión y el recall en una única métrica. Es útil cuando hay un desequilibrio entre las clases, como en este caso. La puntuación F1 para la clase \"1\" es del 15.07%, lo que indica un equilibrio entre la precisión y el recall para esa clase.\n",
    "\n",
    "En cuanto al classification report, proporciona un resumen detallado de las métricas por clase y para el conjunto de datos completo. Observamos que la clase \"0\" (no enfermedad cardíaca) tiene una precisión alta, recall alto y puntuación F1 alta, mientras que la clase \"1\" (enfermedad cardíaca) muestra valores más bajos en todas las métricas. Esto puede indicar un desequilibrio en los datos, donde la clase \"1\" tiene menos representación en comparación con la clase \"0\". El conjunto de datos está sesgado hacia la clase \"0\" y eso puede afectar el rendimiento del modelo en la detección de la clase minoritaria (enfermedad cardíaca).\n",
    "\n",
    "En resumen, las métricas indican que el modelo tiene una precisión general alta, pero tiene dificultades para identificar correctamente los casos de enfermedad cardíaca. Esto puede deberse al desequilibrio en los datos y es importante tenerlo en cuenta al interpretar los resultados y tomar decisiones basadas en ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[57899   468]\n",
      " [ 5124   468]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión \n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred) \n",
    "print(\"Matriz de confusión:\") \n",
    "print(confusion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APLICAREMOS LAS DISTINTAS TÉCNICAS PARA DESBALANCEAR EL PROBLEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "from pylab import rcParams\n",
    " \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    " \n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado aplicaremos diferentes técnicas de desbalanceo de los datos aplicados a un mismo modelo para poder ver cual es la más acertada y así quedarnos con una..............."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Penalización para compensar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos un parámetro adicional en el modelo de Regresión logística en donde indicamos weight = “balanced” y con esto el algoritmo se encargará de equilibrar a la clase minoritaria durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# EscalaMOS las características \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos \n",
    "\n",
    "# Modelo de Logistic Regression\n",
    "logre2 = LogisticRegression(C=0.1,penalty='l1',random_state=42,solver=\"liblinear\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "logre2.fit(X_train_scaled, y_train)\n",
    "y_pred2 = logre2.predict(X_test_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLICACIÓN**\n",
    "\n",
    "En comparación con el modelo anterior, el nuevo modelo de Logistic Regression con los datos de prueba actualizados muestra una mejora en ciertas métricas, mientras que otras disminuyen. Aquí hay un resumen de las diferencias clave:\n",
    "\n",
    "Precisión general: El nuevo modelo tiene una precisión general de 0.74, lo que indica que aproximadamente el 74% de las predicciones son correctas. Esta precisión es menor que la del modelo anterior, que era de 0.91.\n",
    "\n",
    "Precisión por clase: Para la clase \"0\" (no enfermedad cardíaca), la precisión es alta con un valor de 0.97, lo que indica que la mayoría de las predicciones negativas son correctas. Sin embargo, para la clase \"1\" (enfermedad cardíaca), la precisión es baja con un valor de 0.22, lo que indica que la mayoría de las predicciones positivas son incorrectas.\n",
    "\n",
    "Recall por clase: El nuevo modelo muestra un recall alto para la clase \"1\" (enfermedad cardíaca) con un valor de 0.77, lo que indica que la mayoría de los casos reales de enfermedad cardíaca son correctamente identificados por el modelo. Sin embargo, el recall para la clase \"0\" (no enfermedad cardíaca) es de 0.74, lo que indica que hay algunos casos reales de no enfermedad cardíaca que no son identificados por el modelo.\n",
    "\n",
    "Puntuación F1 por clase: La puntuación F1 para la clase \"1\" (enfermedad cardíaca) es de 0.34, lo que indica un equilibrio entre la precisión y el recall para esa clase. Sin embargo, la puntuación F1 para la clase \"0\" (no enfermedad cardíaca) es de 0.84, lo que indica un mejor equilibrio entre precisión y recall para esa clase.\n",
    "\n",
    "En general, el nuevo modelo muestra una mejora en la identificación de casos de enfermedad cardíaca, como se refleja en un recall más alto para la clase \"1\". Sin embargo, la precisión para la clase \"1\" ha disminuido, lo que significa que hay un mayor número de falsos positivos en comparación con el modelo anterior. Además, la precisión general y la puntuación F1 han disminuido. Estos resultados sugieren que el modelo todavía puede beneficiarse de ajustes o considerar técnicas adicionales para abordar el desequilibrio en los datos y mejorar la detección de enfermedades cardíacas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42480 15887]\n",
      " [ 1280  4312]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion2 = confusion_matrix(y_test, y_pred2)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Oversampling de la clase minoritaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, crearemos muestras nuevas “sintéticas” de la clase minoritaria. Usando RandomOverSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 117027})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar RandomOverSampler\n",
    "os = RandomOverSampler(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Imprimir distribución antes y después del resampling\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Definir y entrenar el modelo\n",
    "model = LogisticRegression(C=0.1,penalty='l2',random_state=42,solver=\"newton-cg\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "pred_y_random = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     58367\n",
      "           1       0.21      0.77      0.33      5592\n",
      "\n",
      "    accuracy                           0.73     63959\n",
      "   macro avg       0.59      0.75      0.58     63959\n",
      "weighted avg       0.90      0.73      0.79     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report4 = classification_report(y_test, pred_y_random)\n",
    "print(\"Classification Report:\")\n",
    "print(report4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[42593 15774]\n",
      " [ 1302  4290]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion4 = confusion_matrix(y_test, pred_y_random)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Combinamos resampling con Smote-Tomek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probaremos una técnica muy usada que consiste en aplicar en simultáneo un algoritmo de subsampling y otro de oversampling a la vez al dataset. En este caso usaremos SMOTE para oversampling: busca puntos vecinos cercanos y agrega puntos “en linea recta” entre ellos. Y usaremos Tomek para undersampling que quita los de distinta clase que sean nearest neighbor y deja ver mejor el decisión boundary (la zona limítrofe de nuestras clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes del resampling: Counter({0: 234055, 1: 21781})\n",
      "Distribución después del resampling: Counter({0: 234055, 1: 233355})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from collections import Counter\n",
    "\n",
    "# Aplicar SMOTE para oversampling\n",
    "smote = SMOTE()\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Aplicar Tomek Links para undersampling\n",
    "tomek = TomekLinks()\n",
    "X_train_res, y_train_res = tomek.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Distribución antes del resampling:\", Counter(y_train))\n",
    "print(\"Distribución después del resampling:\", Counter(y_train_res))\n",
    "\n",
    "# Entrenar un modelo (reemplaza esta parte con tu propio código de entrenamiento de modelo)\n",
    "model = LogisticRegression(C=1.0, penalty='l2', random_state=42, solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "pred_y = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.78      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.74      0.56     63959\n",
      "weighted avg       0.90      0.70      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report3 = classification_report(y_test, pred_y)\n",
    "print(\"Classification Report:\")\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[40667 17700]\n",
      " [ 1252  4340]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion3 = confusion_matrix(y_test, pred_y)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia: Ensamble de Modelos con Balanceo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estrategia usaremos un Clasificador de Ensamble que utiliza Bagging y el modelo será un DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=True,\n",
    "                                random_state=0,)\n",
    " \n",
    "#Train the classifier.\n",
    "bbc.fit(X_train, y_train)\n",
    "pred_yBagging = bbc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     58367\n",
      "           1       0.20      0.80      0.32      5592\n",
      "\n",
      "    accuracy                           0.71     63959\n",
      "   macro avg       0.59      0.75      0.57     63959\n",
      "weighted avg       0.91      0.71      0.77     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report4 = classification_report(y_test, pred_yBagging)\n",
    "print(\"Classification Report:\")\n",
    "print(report4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[40827 17540]\n",
      " [ 1118  4474]]\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion4 = confusion_matrix(y_test, pred_yBagging)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de las Estrategias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELOS A TRATAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de este apartado realizaremos la solución de problema a través de diferentes modelos supervisados entre los que nos encontramos con..... en busca de las mejores métricas posibles para nuestro problema......."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (255836, 7)\n",
      "X_test (63959, 7)\n",
      "y_train (255836,)\n",
      "y_test (63959,)\n"
     ]
    }
   ],
   "source": [
    "# El uso de stratify = y es clave para que dicha variable tenga datos balanceados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos los datos de X_test, pero no se entrenan\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos que quiero probar\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(2),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(),\n",
    "    XGBClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics (KNeighborsClassifier):\n",
      "Accuracy: 0.90874\n",
      "Precision: 0.32461\n",
      "Recall: 0.06119\n",
      "F1 Score: 0.10297\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    # Entrenar el clasificador\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Metrics ({classifier.__class__.__name__}):\")\n",
    "    print(f\"Accuracy: {accuracy:.5f}\")\n",
    "    print(f\"Precision: {precision:.5f}\")\n",
    "    print(f\"Recall: {recall:.5f}\")\n",
    "    print(f\"F1 Score: {f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELOS SUPERVISADOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es el aprendizaje supervisado?**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos en sets de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# EscalaMOS las características \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # solo transform no fit(), ya que sino contaminariamos los datos \n",
    "\n",
    "# Modelo de Logistic Regression\n",
    "logre2 = LogisticRegression(C=1.0,penalty='l2',random_state=42,solver=\"newton-cg\", class_weight= \"balanced\") # únicamente incorporamos el class_weight= \"balanced\"\n",
    "logre2.fit(X_train_scaled, y_train)\n",
    "y_pred2 = logre2.predict(X_test_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Crear el pipeline con escalamiento y modelo Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'logre2__C': [0.1, 1, 10],\n",
    "    'logre2__penalty': ['l1', 'l2'], # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # se prueba cada algoritmo\n",
    "    'logre2__class_weight': [None, 'balanced'] # sin balanceo y con balanceo\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     58367\n",
      "           1       0.50      0.08      0.14      5592\n",
      "\n",
      "    accuracy                           0.91     63959\n",
      "   macro avg       0.71      0.54      0.55     63959\n",
      "weighted avg       0.88      0.91      0.88     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred_log = grid_search.predict(X_test)\n",
    "\n",
    "# Calcular el classification report\n",
    "report_log = classification_report(y_test, y_pred_log)\n",
    "print(\"Classification Report:\")\n",
    "print(report_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logre2&#x27;,\n",
       "                 LogisticRegression(C=0.1, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logre2&#x27;,\n",
       "                 LogisticRegression(C=0.1, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('logre2',\n",
       "                 LogisticRegression(C=0.1, penalty='l1', solver='saga'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logre2__C': 0.1,\n",
       " 'logre2__class_weight': None,\n",
       " 'logre2__penalty': 'l1',\n",
       " 'logre2__solver': 'saga'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Crear el pipeline con escalamiento y modelo Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selectkbest', SelectKBest()),\n",
    "    ('logre2', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definir los parámetros a probar en el GridSearchCV\n",
    "parameters = {\n",
    "    'logre2__C': [0.1, 1, 10],\n",
    "    'logre2__penalty': ['l1', 'l2'], # Lasso y Ridge\n",
    "    'logre2__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # se prueba cada algoritmo\n",
    "    'logre2__class_weight': [None, 'balanced'],  # sin balanceo y con balanceo\n",
    "    'selectkbest__k': [5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las predicciones del mejor modelo encontrado\n",
    "y_pred2 = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.69      0.81     58367\n",
      "           1       0.19      0.77      0.31      5592\n",
      "\n",
      "    accuracy                           0.70     63959\n",
      "   macro avg       0.58      0.73      0.56     63959\n",
      "weighted avg       0.90      0.70      0.76     63959\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "{'logre2__C': 0.1, 'logre2__class_weight': 'balanced', 'logre2__penalty': 'l1', 'logre2__solver': 'liblinear', 'selectkbest__k': 5}\n"
     ]
    }
   ],
   "source": [
    "# Calcular el classification report\n",
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(\"Classification Report:\")\n",
    "print(report2)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[0;32m      3\u001b[0m param_grid_lr \u001b[39m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpenalty\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1.0\u001b[39m]\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m grid_search_lr \u001b[39m=\u001b[39m GridSearchCV(lr, param_grid\u001b[39m=\u001b[39mparam_grid_lr, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(lr, param_grid=param_grid_lr, scoring='accuracy', cv=5)\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "best_params_lr = grid_search_lr.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISSION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAUSSIANNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBCCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO NO SUPERVISADO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es el aprendizaje no supervisado?**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN CLASSIIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDES NEURONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
